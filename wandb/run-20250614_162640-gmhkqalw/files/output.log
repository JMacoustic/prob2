EPOCH :      0/100001 | Loss_PDE : 116.6800| Loss_DATA : 2311.0735 | Loss_BC : 0.8674 | RHO : 9.9000 | VIS : 0.020000
EPOCH :   1000/100001 | Loss_PDE : 16.7082| Loss_DATA : 479.1074 | Loss_BC : 223.1372 | RHO : 7.0379 | VIS : -0.074990
EPOCH :   2000/100001 | Loss_PDE : 2.0998| Loss_DATA : 454.1133 | Loss_BC : 222.2200 | RHO : 5.9460 | VIS : -0.177099
EPOCH :   3000/100001 | Loss_PDE : 4.5442| Loss_DATA : 460.2352 | Loss_BC : 191.5420 | RHO : 2.6489 | VIS : -0.219489
EPOCH :   4000/100001 | Loss_PDE : 1.4270| Loss_DATA : 438.2407 | Loss_BC : 115.9395 | RHO : 0.0033 | VIS : -0.013946
EPOCH :   5000/100001 | Loss_PDE : 0.4598| Loss_DATA : 136.5374 | Loss_BC : 40.3747 | RHO : 0.0030 | VIS : -0.000138
EPOCH :   6000/100001 | Loss_PDE : 0.4131| Loss_DATA : 71.5436 | Loss_BC : 12.3454 | RHO : -0.0007 | VIS : 0.000152
EPOCH :   7000/100001 | Loss_PDE : 0.2803| Loss_DATA : 58.6608 | Loss_BC : 11.2109 | RHO : -0.0028 | VIS : -0.000069
EPOCH :   8000/100001 | Loss_PDE : 0.1556| Loss_DATA : 53.0836 | Loss_BC : 8.6235 | RHO : -0.0001 | VIS : 0.000016
EPOCH :   9000/100001 | Loss_PDE : 1.5214| Loss_DATA : 47.8518 | Loss_BC : 4.4480 | RHO : -0.0027 | VIS : -0.000137
EPOCH :  10000/100001 | Loss_PDE : 2.2837| Loss_DATA : 43.8006 | Loss_BC : 2.8643 | RHO : 0.0049 | VIS : 0.000339
EPOCH :  11000/100001 | Loss_PDE : 0.0293| Loss_DATA : 41.5749 | Loss_BC : 2.3324 | RHO : -0.0000 | VIS : -0.000009
EPOCH :  12000/100001 | Loss_PDE : 0.0261| Loss_DATA : 40.2847 | Loss_BC : 2.0993 | RHO : 0.0000 | VIS : -0.000006
EPOCH :  13000/100001 | Loss_PDE : 6.0467| Loss_DATA : 39.8486 | Loss_BC : 2.1363 | RHO : 0.0057 | VIS : 0.000163
EPOCH :  14000/100001 | Loss_PDE : 0.0493| Loss_DATA : 38.9909 | Loss_BC : 1.8170 | RHO : -0.0004 | VIS : 0.000005
EPOCH :  15000/100001 | Loss_PDE : 0.4532| Loss_DATA : 38.7357 | Loss_BC : 1.8002 | RHO : 0.0011 | VIS : -0.000036
EPOCH :  16000/100001 | Loss_PDE : 2.0940| Loss_DATA : 38.5434 | Loss_BC : 1.8194 | RHO : -0.0057 | VIS : -0.000092
EPOCH :  17000/100001 | Loss_PDE : 0.0311| Loss_DATA : 38.2867 | Loss_BC : 1.7308 | RHO : 0.0001 | VIS : -0.000022
EPOCH :  18000/100001 | Loss_PDE : 0.0527| Loss_DATA : 38.1187 | Loss_BC : 1.7082 | RHO : 0.0005 | VIS : -0.000039
EPOCH :  19000/100001 | Loss_PDE : 0.2183| Loss_DATA : 37.9624 | Loss_BC : 1.6827 | RHO : 0.0015 | VIS : 0.000010
EPOCH :  20000/100001 | Loss_PDE : 4.0620| Loss_DATA : 37.8519 | Loss_BC : 1.6529 | RHO : 0.0058 | VIS : 0.000086
EPOCH :  21000/100001 | Loss_PDE : 0.1254| Loss_DATA : 41.3453 | Loss_BC : 3.3130 | RHO : -0.0000 | VIS : -0.000005
EPOCH :  22000/100001 | Loss_PDE : 0.0654| Loss_DATA : 38.6482 | Loss_BC : 2.3023 | RHO : -0.0000 | VIS : -0.000018
EPOCH :  23000/100001 | Loss_PDE : 24.6691| Loss_DATA : 38.9067 | Loss_BC : 2.3296 | RHO : -0.0000 | VIS : 0.000441
EPOCH :  24000/100001 | Loss_PDE : 0.0366| Loss_DATA : 38.0893 | Loss_BC : 1.7441 | RHO : -0.0000 | VIS : -0.000007
EPOCH :  25000/100001 | Loss_PDE : 0.0334| Loss_DATA : 37.9586 | Loss_BC : 1.6775 | RHO : -0.0000 | VIS : -0.000007
Traceback (most recent call last):
  File "/root/prob2/train.py", line 127, in <module>
    loss.backward()
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
