EPOCH :      0/100001 | Loss_PDE : 1252.3142| Loss_DATA : 2311.0735 | Loss_BC : 0.8674 | RHO : 99.9900 | VIS : 0.011000
EPOCH :   1000/100001 | Loss_PDE : 11.9465| Loss_DATA : 938.2459 | Loss_BC : 67.7328 | RHO : 99.8158 | VIS : 0.004005
EPOCH :   2000/100001 | Loss_PDE : 5.4095| Loss_DATA : 576.0318 | Loss_BC : 169.5325 | RHO : 99.6961 | VIS : -0.006621
EPOCH :   3000/100001 | Loss_PDE : 1.5340| Loss_DATA : 463.5088 | Loss_BC : 238.2834 | RHO : 99.6274 | VIS : -0.013826
EPOCH :   4000/100001 | Loss_PDE : 1.2351| Loss_DATA : 484.2043 | Loss_BC : 220.1143 | RHO : 99.5729 | VIS : -0.019067
EPOCH :   5000/100001 | Loss_PDE : 0.2945| Loss_DATA : 454.3344 | Loss_BC : 245.3176 | RHO : 99.5267 | VIS : -0.023569
EPOCH :   6000/100001 | Loss_PDE : 0.1931| Loss_DATA : 448.6072 | Loss_BC : 250.9894 | RHO : 99.4741 | VIS : -0.026659
EPOCH :   7000/100001 | Loss_PDE : 0.8095| Loss_DATA : 456.6713 | Loss_BC : 241.6288 | RHO : 99.3916 | VIS : -0.028074
EPOCH :   8000/100001 | Loss_PDE : 0.1461| Loss_DATA : 451.7071 | Loss_BC : 244.4249 | RHO : 99.3643 | VIS : -0.027028
EPOCH :   9000/100001 | Loss_PDE : 0.1576| Loss_DATA : 449.6527 | Loss_BC : 243.8456 | RHO : 99.2597 | VIS : -0.023261
EPOCH :  10000/100001 | Loss_PDE : 218.5178| Loss_DATA : 1838.5194 | Loss_BC : 4.9975 | RHO : 98.0466 | VIS : -0.020534
EPOCH :  11000/100001 | Loss_PDE : 152.1375| Loss_DATA : 1764.3351 | Loss_BC : 6.9712 | RHO : 98.0466 | VIS : -0.006725
EPOCH :  12000/100001 | Loss_PDE : 168.9896| Loss_DATA : 1420.9938 | Loss_BC : 26.4248 | RHO : 98.0466 | VIS : 0.024539
EPOCH :  13000/100001 | Loss_PDE : 161.7695| Loss_DATA : 808.4899 | Loss_BC : 108.5703 | RHO : 98.0466 | VIS : 0.037720
EPOCH :  14000/100001 | Loss_PDE : 316.1655| Loss_DATA : 943.9076 | Loss_BC : 67.3628 | RHO : 98.0447 | VIS : 0.052808
EPOCH :  15000/100001 | Loss_PDE : 21.6827| Loss_DATA : 1101.6613 | Loss_BC : 45.3710 | RHO : 98.0332 | VIS : 0.061424
EPOCH :  16000/100001 | Loss_PDE : 1028.9865| Loss_DATA : 1624.3884 | Loss_BC : 17.1128 | RHO : 97.1603 | VIS : 0.072973
EPOCH :  17000/100001 | Loss_PDE : 434.9991| Loss_DATA : 1645.7190 | Loss_BC : 13.8387 | RHO : 97.1603 | VIS : 0.072721
EPOCH :  18000/100001 | Loss_PDE : 248.0603| Loss_DATA : 1644.6530 | Loss_BC : 11.8317 | RHO : 97.1603 | VIS : 0.072488
EPOCH :  19000/100001 | Loss_PDE : 147.9572| Loss_DATA : 1610.0061 | Loss_BC : 10.8478 | RHO : 97.1603 | VIS : 0.072140
EPOCH :  20000/100001 | Loss_PDE : 98.7880| Loss_DATA : 1527.6884 | Loss_BC : 11.6014 | RHO : 97.1603 | VIS : 0.071614
EPOCH :  21000/100001 | Loss_PDE : 79.3834| Loss_DATA : 1399.9946 | Loss_BC : 15.5279 | RHO : 97.1603 | VIS : 0.070726
EPOCH :  22000/100001 | Loss_PDE : 69.9301| Loss_DATA : 1234.1134 | Loss_BC : 24.9471 | RHO : 97.1603 | VIS : 0.069343
EPOCH :  23000/100001 | Loss_PDE : 58.7794| Loss_DATA : 1029.0878 | Loss_BC : 44.1425 | RHO : 97.1603 | VIS : 0.067570
EPOCH :  24000/100001 | Loss_PDE : 50.1700| Loss_DATA : 783.3362 | Loss_BC : 84.1610 | RHO : 97.1603 | VIS : 0.065298
EPOCH :  25000/100001 | Loss_PDE : 37.1025| Loss_DATA : 597.7608 | Loss_BC : 143.0980 | RHO : 97.1603 | VIS : 0.062838
EPOCH :  26000/100001 | Loss_PDE : 25.8878| Loss_DATA : 585.7965 | Loss_BC : 150.3682 | RHO : 97.1587 | VIS : 0.062809
EPOCH :  27000/100001 | Loss_PDE : 30.4562| Loss_DATA : 612.9678 | Loss_BC : 142.8438 | RHO : 97.1545 | VIS : 0.066193
EPOCH :  28000/100001 | Loss_PDE : 25.8387| Loss_DATA : 558.7238 | Loss_BC : 171.2769 | RHO : 97.1486 | VIS : 0.065464
EPOCH :  29000/100001 | Loss_PDE : 8.5508| Loss_DATA : 963.9617 | Loss_BC : 58.3543 | RHO : 97.1281 | VIS : 0.066378
EPOCH :  30000/100001 | Loss_PDE : 310.2838| Loss_DATA : 2495.1621 | Loss_BC : 2.3900 | RHO : 96.2228 | VIS : 0.075376
EPOCH :  31000/100001 | Loss_PDE : 175.7979| Loss_DATA : 1923.1532 | Loss_BC : 4.0115 | RHO : 96.2228 | VIS : 0.075907
EPOCH :  32000/100001 | Loss_PDE : 192.6462| Loss_DATA : 1394.8894 | Loss_BC : 33.7244 | RHO : 96.2228 | VIS : 0.073988
EPOCH :  33000/100001 | Loss_PDE : 138.9488| Loss_DATA : 1007.4525 | Loss_BC : 80.5855 | RHO : 96.2228 | VIS : 0.072364
EPOCH :  34000/100001 | Loss_PDE : 99.4946| Loss_DATA : 748.5325 | Loss_BC : 133.5918 | RHO : 96.2228 | VIS : 0.072060
EPOCH :  35000/100001 | Loss_PDE : 71.4578| Loss_DATA : 607.0653 | Loss_BC : 173.6400 | RHO : 96.2228 | VIS : 0.073720
EPOCH :  36000/100001 | Loss_PDE : 44.3577| Loss_DATA : 522.7712 | Loss_BC : 203.1776 | RHO : 96.2228 | VIS : 0.075728
EPOCH :  37000/100001 | Loss_PDE : 26.3906| Loss_DATA : 485.3020 | Loss_BC : 218.4274 | RHO : 96.2224 | VIS : 0.078225
EPOCH :  38000/100001 | Loss_PDE : 13.7204| Loss_DATA : 468.7271 | Loss_BC : 228.6106 | RHO : 96.2213 | VIS : 0.079464
EPOCH :  39000/100001 | Loss_PDE : 36.3672| Loss_DATA : 458.9889 | Loss_BC : 237.1762 | RHO : 96.2205 | VIS : 0.077363
EPOCH :  40000/100001 | Loss_PDE : 5.6938| Loss_DATA : 453.0785 | Loss_BC : 241.2696 | RHO : 96.2189 | VIS : 0.069233
EPOCH :  41000/100001 | Loss_PDE : 4.2174| Loss_DATA : 447.3542 | Loss_BC : 240.8754 | RHO : 96.2162 | VIS : 0.058160
EPOCH :  42000/100001 | Loss_PDE : 13.2824| Loss_DATA : 446.7911 | Loss_BC : 230.8979 | RHO : 96.2097 | VIS : 0.053255
EPOCH :  43000/100001 | Loss_PDE : 99.2324| Loss_DATA : 1824.8678 | Loss_BC : 2.0759 | RHO : 95.1643 | VIS : 0.049379
EPOCH :  44000/100001 | Loss_PDE : 169.8203| Loss_DATA : 1537.6758 | Loss_BC : 11.1707 | RHO : 95.1643 | VIS : 0.046971
EPOCH :  45000/100001 | Loss_PDE : 202.5798| Loss_DATA : 1362.7108 | Loss_BC : 20.2401 | RHO : 95.1643 | VIS : 0.040235
EPOCH :  46000/100001 | Loss_PDE : 182.4973| Loss_DATA : 1033.6027 | Loss_BC : 53.7839 | RHO : 95.1643 | VIS : 0.032759
EPOCH :  47000/100001 | Loss_PDE : 53.4811| Loss_DATA : 609.8790 | Loss_BC : 155.5882 | RHO : 95.1643 | VIS : 0.027019
EPOCH :  48000/100001 | Loss_PDE : 25.8494| Loss_DATA : 478.5877 | Loss_BC : 212.2818 | RHO : 95.1643 | VIS : 0.020159
Traceback (most recent call last):
  File "/root/prob2/train.py", line 127, in <module>
    loss.backward()
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
