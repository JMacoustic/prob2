net.0.weight grad norm: 1.39e+00
net.0.bias grad norm: 2.17e+00
net.2.weight grad norm: 1.93e+01
net.2.bias grad norm: 4.31e+00
net.4.weight grad norm: 2.07e+01
net.4.bias grad norm: 7.59e+00
net.6.weight grad norm: 2.14e+01
net.6.bias grad norm: 1.29e+01
net.8.weight grad norm: 2.45e+01
net.8.bias grad norm: 2.30e+01
net.10.weight grad norm: 3.01e+01
net.10.bias grad norm: 3.76e+01
net.0.weight grad norm: 5.06e-01
net.0.bias grad norm: 9.97e-01
net.2.weight grad norm: 9.89e+00
net.2.bias grad norm: 2.04e+00
net.4.weight grad norm: 9.99e+00
net.4.bias grad norm: 3.57e+00
net.6.weight grad norm: 9.73e+00
net.6.bias grad norm: 6.02e+00
net.8.weight grad norm: 9.17e+00
net.8.bias grad norm: 9.78e+00
net.10.weight grad norm: 1.27e+01
net.10.bias grad norm: 1.75e+01
rho grad: 0.0004347561625763774
vis grad: -0.0015706766862422228
EPOCH :      0/  3001 | Loss_PDE : 0.0027| Loss_DATA : 66.3666 | RHO : 9.9900 | VIS : 0.005010
net.0.weight grad norm: 6.04e-01
net.0.bias grad norm: 1.46e+00
net.2.weight grad norm: 1.09e+01
net.2.bias grad norm: 2.48e+00
net.4.weight grad norm: 4.01e+02
net.4.bias grad norm: 3.73e+01
net.6.weight grad norm: 3.47e+02
net.6.bias grad norm: 6.50e+01
net.8.weight grad norm: 1.20e+02
net.8.bias grad norm: 2.10e+01
net.10.weight grad norm: 9.89e+01
net.10.bias grad norm: 2.80e+01
net.0.weight grad norm: 2.58e+00
net.0.bias grad norm: 2.19e+00
net.2.weight grad norm: 1.83e+01
net.2.bias grad norm: 3.39e+00
net.4.weight grad norm: 6.14e+01
net.4.bias grad norm: 2.50e+00
net.6.weight grad norm: 5.87e+01
net.6.bias grad norm: 3.64e+00
net.8.weight grad norm: 8.79e+00
net.8.bias grad norm: 1.02e+00
net.10.weight grad norm: 7.48e+00
net.10.bias grad norm: 1.23e+00
rho grad: 0.05513620749115944
vis grad: 0.14623421430587769
net.0.weight grad norm: 3.89e-01
net.0.bias grad norm: 5.72e-01
net.2.weight grad norm: 1.20e+00
net.2.bias grad norm: 2.39e-01
net.4.weight grad norm: 6.31e+00
net.4.bias grad norm: 5.45e-01
net.6.weight grad norm: 1.03e+02
net.6.bias grad norm: 9.08e+00
net.8.weight grad norm: 5.59e+01
net.8.bias grad norm: 5.88e+00
net.10.weight grad norm: 8.02e+02
net.10.bias grad norm: 7.25e+01
net.0.weight grad norm: 1.11e+01
net.0.bias grad norm: 1.73e+01
net.2.weight grad norm: 4.78e+01
net.2.bias grad norm: 1.05e+01
net.4.weight grad norm: 9.70e+01
net.4.bias grad norm: 1.00e+01
net.6.weight grad norm: 8.17e+01
net.6.bias grad norm: 7.76e+00
net.8.weight grad norm: 5.82e+01
net.8.bias grad norm: 5.97e+00
net.10.weight grad norm: 3.62e+02
net.10.bias grad norm: 3.39e+01
rho grad: 0.04174729809165001
vis grad: 0.016125192865729332
net.0.weight grad norm: 1.11e-04
net.0.bias grad norm: 2.60e-04
net.2.weight grad norm: 1.33e-03
net.2.bias grad norm: 2.98e-04
net.4.weight grad norm: 1.75e-02
net.4.bias grad norm: 1.62e-03
net.6.weight grad norm: 2.61e-02
net.6.bias grad norm: 2.31e-03
net.8.weight grad norm: 4.23e-02
net.8.bias grad norm: 4.16e-03
net.10.weight grad norm: 3.24e+02
net.10.bias grad norm: 2.87e+01
net.0.weight grad norm: 2.16e-07
net.0.bias grad norm: 5.91e-07
net.2.weight grad norm: 8.19e-07
net.2.bias grad norm: 1.73e-07
net.4.weight grad norm: 4.41e-06
net.4.bias grad norm: 3.89e-07
net.6.weight grad norm: 1.23e-04
net.6.bias grad norm: 1.09e-05
net.8.weight grad norm: 2.65e-04
net.8.bias grad norm: 2.43e-05
net.10.weight grad norm: 1.92e+02
net.10.bias grad norm: 1.70e+01
rho grad: 1.073569364962168e-06
vis grad: 5.0517698468866e-08
net.0.weight grad norm: 4.96e-08
net.0.bias grad norm: 1.53e-07
net.2.weight grad norm: 6.55e-07
net.2.bias grad norm: 1.43e-07
net.4.weight grad norm: 6.93e-06
net.4.bias grad norm: 6.47e-07
net.6.weight grad norm: 1.41e-04
net.6.bias grad norm: 1.25e-05
net.8.weight grad norm: 5.95e-04
net.8.bias grad norm: 5.44e-05
net.10.weight grad norm: 2.42e+02
net.10.bias grad norm: 2.13e+01
net.0.weight grad norm: 5.53e-07
net.0.bias grad norm: 1.50e-06
net.2.weight grad norm: 1.86e-06
net.2.bias grad norm: 4.07e-07
net.4.weight grad norm: 6.59e-06
net.4.bias grad norm: 6.09e-07
net.6.weight grad norm: 4.51e-05
net.6.bias grad norm: 4.03e-06
net.8.weight grad norm: 3.48e-04
net.8.bias grad norm: 3.13e-05
net.10.weight grad norm: 3.19e+02
net.10.bias grad norm: 2.82e+01
rho grad: 1.204610846836096e-12
vis grad: 3.8000139601819527e-13
net.0.weight grad norm: 8.42e-09
net.0.bias grad norm: 4.77e-08
net.2.weight grad norm: 1.61e-07
net.2.bias grad norm: 3.38e-08
net.4.weight grad norm: 1.48e-06
net.4.bias grad norm: 1.39e-07
net.6.weight grad norm: 4.46e-04
net.6.bias grad norm: 3.94e-05
net.8.weight grad norm: 5.67e-03
net.8.bias grad norm: 5.10e-04
net.10.weight grad norm: 2.33e+02
net.10.bias grad norm: 2.06e+01
net.0.weight grad norm: 3.19e-06
net.0.bias grad norm: 7.98e-06
net.2.weight grad norm: 7.93e-06
net.2.bias grad norm: 1.71e-06
net.4.weight grad norm: 1.51e-05
net.4.bias grad norm: 1.40e-06
net.6.weight grad norm: 3.94e-05
net.6.bias grad norm: 3.52e-06
net.8.weight grad norm: 2.72e-04
net.8.bias grad norm: 2.41e-05
net.10.weight grad norm: 1.59e+02
net.10.bias grad norm: 1.40e+01
rho grad: 2.3188455279196662e-14
vis grad: 1.2786574743624528e-14
net.0.weight grad norm: 2.94e-09
net.0.bias grad norm: 1.81e-08
net.2.weight grad norm: 5.15e-08
net.2.bias grad norm: 1.06e-08
net.4.weight grad norm: 4.58e-07
net.4.bias grad norm: 4.30e-08
net.6.weight grad norm: 1.23e-03
net.6.bias grad norm: 1.08e-04
net.8.weight grad norm: 1.17e-02
net.8.bias grad norm: 1.05e-03
net.10.weight grad norm: 2.80e+02
net.10.bias grad norm: 2.47e+01
net.0.weight grad norm: 5.54e-06
net.0.bias grad norm: 1.55e-05
net.2.weight grad norm: 1.23e-05
net.2.bias grad norm: 2.73e-06
net.4.weight grad norm: 3.32e-05
net.4.bias grad norm: 3.10e-06
net.6.weight grad norm: 1.42e-04
net.6.bias grad norm: 1.26e-05
net.8.weight grad norm: 1.34e-04
net.8.bias grad norm: 8.40e-06
net.10.weight grad norm: 8.46e+01
net.10.bias grad norm: 7.48e+00
rho grad: 1.6422274781366375e-14
vis grad: 1.353790906146509e-14
Traceback (most recent call last):
  File "C:\Users\juhye\Desktop\Current Files\csmproject\prob2\train.py", line 118, in <module>
    loss.backward()
  File "C:\Users\juhye\.pyenv\pyenv-win\versions\3.12.8\Lib\site-packages\torch\_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "C:\Users\juhye\.pyenv\pyenv-win\versions\3.12.8\Lib\site-packages\torch\autograd\__init__.py", line 347, in backward
    _engine_run_backward(
  File "C:\Users\juhye\.pyenv\pyenv-win\versions\3.12.8\Lib\site-packages\torch\autograd\graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
